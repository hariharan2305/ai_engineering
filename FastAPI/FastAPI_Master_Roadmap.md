# FastAPI Master Roadmap for GenAI Backend Development

**Your Goal:** Master building production-grade backends for GenAI applications using FastAPI, with multi-provider LLM support (Anthropic, OpenAI, LiteLLM, OpenRouter), all deployable via Docker.

**Your Background:** Senior ML Engineer (6 years) with Python, SQL, Databricks, PySpark, and AWS expertise. This roadmap is tailored to your experience level‚Äîno oversimplification, with analogies connecting to your ML infrastructure background.

---

## üìä Learning Strategy

This roadmap follows a **Hybrid Approach** that balances:
1. **Foundational Concepts** ‚Üí Build core FastAPI skills
2. **Production Readiness** ‚Üí Inject database, testing, deployment patterns early
3. **GenAI Evolution** ‚Üí Progressive LLM integration from simple chat ‚Üí agents

**Time Estimate:** 8-12 weeks (assuming 10-15 hours/week)

---

## üéØ Roadmap Overview

```
Phase 1: Foundation (Week 1)
‚îú‚îÄ ‚úÖ Topic 1: HTTP & REST API Basics
‚îú‚îÄ ‚úÖ Topic 2: FastAPI Core Concepts
‚îî‚îÄ üìå Advanced Extensions (1.3.5, 1.4.4) - Revisit later

Phase 2: Backend Essentials (Week 2-3)
‚îú‚îÄ Topic 3: Error Handling & Custom Responses
‚îú‚îÄ Topic 4: Middleware & Request Lifecycle
‚îú‚îÄ Topic 5: Async FastAPI for Concurrent Operations
‚îî‚îÄ Topic 6: Project Structure & Organization ‚≠ê NEW

Phase 3: Production Foundations (Week 4-5)
‚îú‚îÄ Topic 7: Database Integration (SQLAlchemy + PostgreSQL + Redis)
‚îú‚îÄ Topic 8: Testing FastAPI Applications
‚îî‚îÄ üîß Mini-Project 1: Chat API with Database & Tests

Phase 4: LLM Integration Basics (Week 6)
‚îú‚îÄ Topic 9: Integrating Real LLM Providers
‚îú‚îÄ Topic 10: Streaming Responses & Server-Sent Events
‚îî‚îÄ üîß Mini-Project 2: Multi-Provider Chat with History

Phase 5: Authentication & Security (Week 7)
‚îú‚îÄ Topic 11: Authentication & API Keys
‚îú‚îÄ Topic 12: Rate Limiting & Abuse Prevention
‚îî‚îÄ üîß Mini-Project 3: Secure Chat API with Usage Limits

Phase 6: Advanced LLM Patterns (Week 8-9)
‚îú‚îÄ Topic 13: Advanced Multi-Provider Patterns
‚îú‚îÄ Topic 14: GenAI-Specific Patterns (Prompts, Context, Caching)
‚îú‚îÄ Topic 15: Function Calling & Tool Use
‚îî‚îÄ üîß Mini-Project 4: Agentic RAG System with Tools

Phase 7: Production Deployment (Week 10-11)
‚îú‚îÄ Topic 16: Production Docker (Multi-stage, Security)
‚îú‚îÄ Topic 17: Production Deployment (Gunicorn, NGINX, Health Checks)
‚îú‚îÄ Topic 18: Monitoring & Metrics (Prometheus, Logging)
‚îî‚îÄ üîß Mini-Project 5: Production-Ready Deployment

Phase 8: Advanced Topics (Week 12+)
‚îú‚îÄ Topic 19: Vector Databases for RAG
‚îú‚îÄ Topic 20: Distributed Tracing (OpenTelemetry)
‚îú‚îÄ Topic 21: Container Orchestration (Kubernetes Basics)
‚îú‚îÄ Topic 22: LangChain & LangServe Integration ‚≠ê NEW (Optional)
‚îî‚îÄ üîß Final Project: Enterprise GenAI Platform
```

**Total:** 22 Topics + 6 Mini-Projects + 1 Final Project

---

## üìö Detailed Roadmap

### **PHASE 1: Foundation** ‚úÖ **COMPLETED**

#### ‚úÖ Topic 1: HTTP & REST API Basics
**File:** `concepts/01_HTTP_REST_API_Basics.md`

**What You Learned:**
- HTTP protocol fundamentals (request/response cycle)
- HTTP methods (GET, POST, PUT, PATCH, DELETE) mapped to SQL operations
- Status codes (2xx, 4xx, 5xx) and when to use each
- REST principles: statelessness, client-server, uniform interface
- Why statelessness matters for load balancers in distributed systems
- RESTful URL design patterns

**Key Takeaway:** LLM APIs are stateless‚Äîevery request includes full conversation history because load balancers route to different servers (like Spark job scheduling across cluster nodes).

---

#### ‚úÖ Topic 2: FastAPI Core Concepts
**Files:**
- `concepts/02_FastAPI_Core_Concepts.md`
- `concepts/02_FastAPI_Hands_On_Practice.md`

**What You Learned:**
- Application setup with FastAPI() and Uvicorn
- Path operations with decorators (@app.get, @app.post)
- Path parameters with type validation and Enums
- Query parameters with defaults and constraints
- Request body handling with Pydantic models
- Response models and HTTP status codes
- Automatic interactive documentation (/docs)
- Nested Pydantic models and field validation

**Key Exercises Completed:**
1. Hello GenAI World (basic server)
2. Path Parameters (type validation, enums)
3. Query Parameters (pagination, filtering)
4. Request Bodies (Pydantic models, constraints)
5. HTTP Status Codes (201, 204, 404, 422)
6. Complete Mini GenAI API

**Key Takeaway:** Type hints = automatic validation. FastAPI does all the heavy lifting‚Äîyou never see invalid data.

**Advanced Extensions (Revisit After Phase 3):**

<details>
<summary>üìå 1.3.5 Custom Base Model Pattern (Industry Best Practice)</summary>

**What Top GenAI Companies Do:**
- [ ] Creating app-wide `CustomModel` base class for consistent behavior
- [ ] Standardized datetime serialization (ISO format everywhere)
- [ ] Custom methods shared across all schemas (e.g., `to_dict()`, `from_orm()`)
- [ ] Config class for global model settings (JSON encoders, ORM mode)
- [ ] Centralized validation error formatting

**Example Pattern:**
```python
from pydantic import BaseModel
from datetime import datetime

class CustomModel(BaseModel):
    class Config:
        from_attributes = True  # Pydantic v2
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }

    def to_dict(self):
        return self.model_dump()
```

**ML Engineer Analogy:** Like defining a base feature transformer class that standardizes how all features are processed and serialized across your ML pipeline.

</details>

<details>
<summary>üìå 1.4.4 Advanced Dependency Patterns (Industry Best Practice)</summary>

**What Top GenAI Companies Do:**
- [ ] Dependency chaining for complex validation (chain multiple deps together)
- [ ] Request-scoped caching behavior (FastAPI caches dep results within a single request)
- [ ] Dependencies as validators (business logic validation, not just injection)
- [ ] Async dependencies with database queries
- [ ] Sub-dependencies for shared logic

**Key Insight:** FastAPI caches dependency results within a single request. If you call `get_current_user()` in three places, it only executes once per request.

**Example Pattern:**
```python
# Dependency chaining: verify_token ‚Üí get_user ‚Üí verify_admin
async def get_current_user(token: str = Depends(verify_token)):
    return await fetch_user(token.user_id)

async def get_admin_user(user: User = Depends(get_current_user)):
    if not user.is_admin:
        raise HTTPException(403, "Admin required")
    return user
```

**ML Engineer Analogy:** Like Spark transformation chaining where each step depends on the previous, but intermediate results are cached (like `.cache()` on an RDD).

</details>

---

### **PHASE 2: Backend Essentials** üéØ **CURRENT FOCUS**

#### üìñ Topic 3: Error Handling & Custom Responses
**Target Files:**
- `concepts/03_Error_Handling.md`
- `concepts/03_Error_Handling_Practice.md`

**Learning Objectives:**
- Understanding FastAPI's exception handling architecture
- Using HTTPException for standard errors
- Creating custom exception classes
- Implementing global exception handlers
- Validation error customization
- Error response models with Pydantic
- Logging errors for debugging and monitoring
- Handling errors in async functions
- Error handling in streaming responses

**Hands-On:**
- Exercise 1: Basic HTTPException usage (404, 400, 403)
- Exercise 2: Custom exception classes
- Exercise 3: Global exception handlers
- Exercise 4: Validation error customization
- Exercise 5: Error logging integration
- Exercise 6: Building error-aware GenAI API

**ML Engineer Analogy:** Exception handlers = data quality checks in ETL pipelines. Catch bad data early, log it, and return meaningful errors instead of crashing.

**Estimated Time:** 2 hours concepts + 3 hours hands-on

---

#### üìñ Topic 4: Middleware & Request Lifecycle
**Target Files:**
- `concepts/04_Middleware.md`
- `concepts/04_Middleware_Practice.md`

**Learning Objectives:**
- Understanding the request-response lifecycle
- Built-in middleware (CORS, Trusted Host, HTTPS redirect)
- Creating custom middleware
- Request timing and logging middleware
- Authentication middleware
- Request ID tracking for distributed systems
- Middleware ordering and execution flow
- Error handling in middleware
- Performance considerations

**Hands-On:**
- Exercise 1: CORS configuration for frontend integration
- Exercise 2: Request timing middleware
- Exercise 3: Request ID injection
- Exercise 4: Authentication middleware
- Exercise 5: Custom logging middleware
- Exercise 6: Chaining multiple middleware

**ML Engineer Analogy:** Middleware = Apache Spark transformations in a pipeline. Each middleware transforms the request before it reaches your endpoint, just like .map() and .filter() transform RDDs.

**Estimated Time:** 2 hours concepts + 3 hours hands-on

---

#### üìñ Topic 5: Async FastAPI for Concurrent Operations
**Target Files:**
- `concepts/05_Async_FastAPI.md`
- `concepts/05_Async_FastAPI_Practice.md`

**Learning Objectives:**
- async/await fundamentals in Python
- When to use async vs sync in FastAPI
- Concurrent API calls with asyncio.gather()
- Async database operations
- Async HTTP clients (httpx)
- Event loop concepts
- Avoiding blocking operations
- Async context managers
- Error handling in async functions
- Performance benchmarking: async vs sync

**Hands-On:**
- Exercise 1: Converting sync to async endpoints
- Exercise 2: Concurrent LLM provider calls
- Exercise 3: Async HTTP client integration (httpx)
- Exercise 4: Parallel database queries
- Exercise 5: Timeouts and cancellation
- Exercise 6: Multi-provider GenAI API with concurrent fallback

**ML Engineer Analogy:** Async = I/O parallelism (waiting for APIs/databases), not CPU parallelism (like Spark). Think of it as waiting for multiple S3 downloads simultaneously instead of sequentially.

**Critical Insight:** LLM API calls are I/O-bound (waiting for network). Async lets you make 10 concurrent requests with 10 different users instead of blocking on each.

**Estimated Time:** 2.5 hours concepts + 4 hours hands-on

---

#### üìñ Topic 6: Project Structure & Organization
**Target Files:**
- `concepts/06_Project_Structure.md`
- `concepts/06_Project_Structure_Practice.md`

**Learning Objectives:**

**Part 1: Domain-Driven Structure**
- Why organize by domain/feature, not by file type
- Creating self-contained domain packages
- Router mounting patterns
- Module-specific configuration classes
- Avoiding circular imports

**Part 2: Production Project Layout**
```
app/
‚îú‚îÄ‚îÄ main.py                 # Application entry point
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ config.py           # Settings using Pydantic
‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py     # Shared dependencies
‚îÇ   ‚îî‚îÄ‚îÄ exceptions.py       # Custom exceptions
‚îú‚îÄ‚îÄ auth/
‚îÇ   ‚îú‚îÄ‚îÄ router.py           # Auth endpoints
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py          # Auth Pydantic models
‚îÇ   ‚îú‚îÄ‚îÄ models.py           # Auth SQLAlchemy models
‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py     # Auth-specific deps
‚îÇ   ‚îî‚îÄ‚îÄ service.py          # Auth business logic
‚îú‚îÄ‚îÄ chat/
‚îÇ   ‚îú‚îÄ‚îÄ router.py           # Chat endpoints
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py          # Chat Pydantic models
‚îÇ   ‚îú‚îÄ‚îÄ models.py           # Chat SQLAlchemy models
‚îÇ   ‚îî‚îÄ‚îÄ service.py          # Chat business logic
‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îú‚îÄ‚îÄ router.py           # LLM provider endpoints
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py          # LLM request/response models
‚îÇ   ‚îú‚îÄ‚îÄ providers/          # Provider implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anthropic.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ openai.py
‚îÇ   ‚îî‚îÄ‚îÄ service.py          # LLM orchestration
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ conftest.py         # Shared fixtures
    ‚îú‚îÄ‚îÄ auth/
    ‚îú‚îÄ‚îÄ chat/
    ‚îî‚îÄ‚îÄ llm/
```

**Part 3: Router Organization**
- Prefixes and tags for automatic grouping
- Include vs APIRouter patterns
- Response models per router
- Dependency injection at router level

**Part 4: Configuration Patterns**
- Pydantic Settings for environment variables
- Module-specific config classes
- Configuration validation on startup
- Multi-environment support (dev, staging, prod)

**Part 5: Shared Utilities**
- When to create utils vs keeping code local
- Avoiding "utils hell" (over-abstraction)
- Type aliases and common types
- Constants and enums organization

**Hands-On:**
- Exercise 1: Refactoring flat structure to domain-driven
- Exercise 2: Creating modular router architecture
- Exercise 3: Pydantic Settings for configuration
- Exercise 4: Environment-specific configurations
- Exercise 5: Module-specific dependencies
- Exercise 6: Complete GenAI API with proper structure

**ML Engineer Analogy:** Domain-driven structure = organizing ML code by feature pipelines (training/, serving/, feature_engineering/) rather than by file type (models.py, utils.py, config.py everywhere). Each domain is like a self-contained ML workflow.

**Why This Matters for GenAI Apps:**
- LLM providers, auth, chat, RAG are distinct domains
- Each domain has its own schemas, business logic, and dependencies
- Clean separation enables team collaboration and testing
- New providers or features slot into existing structure

**Estimated Time:** 2 hours concepts + 3 hours hands-on

---

### **PHASE 3: Production Foundations** üîß **DATABASE + TESTING**

#### üìñ Topic 7: Database Integration
**Target Files:**
- `concepts/07_Database_Integration.md`
- `concepts/07_Database_Integration_Practice.md`

**Learning Objectives:**

**Part 1: PostgreSQL with SQLAlchemy**
- Database selection guidance (PostgreSQL vs MongoDB vs DynamoDB)
- SQLAlchemy ORM fundamentals
- Async SQLAlchemy setup
- Database session management with dependencies
- Defining models (User, Conversation, Message)
- Relationships and foreign keys
- Querying patterns
- Alembic migrations
- Connection pooling
- Transaction management

**Part 2: Redis Patterns**
- Redis setup and connection
- Caching LLM responses
- Rate limiting with Redis
- Session storage
- Pub/sub for real-time features
- Redis connection pooling
- TTL strategies

**Part 3: Schema Design for GenAI Apps**
- Users table (auth, preferences)
- Conversations table (metadata, participants)
- Messages table (role, content, tokens)
- Token usage tracking
- Feedback/ratings schema
- Audit logs

**Hands-On:**
- Exercise 1: SQLAlchemy models for chat app
- Exercise 2: Database session dependency
- Exercise 3: CRUD operations (Create, Read, Update, Delete)
- Exercise 4: Conversation retrieval with pagination
- Exercise 5: Redis caching layer
- Exercise 6: Rate limiting with Redis
- Exercise 7: Alembic migrations
- Exercise 8: Complete chat API with PostgreSQL + Redis

**ML Engineer Analogy:**
- PostgreSQL = Delta Lake (structured, transactional)
- Redis = Spark cache() (fast, in-memory)
- Alembic migrations = schema evolution in Delta

**Estimated Time:** 4 hours concepts + 6 hours hands-on

---

#### üìñ Topic 8: Testing FastAPI Applications
**Target Files:**
- `concepts/08_Testing.md`
- `concepts/08_Testing_Practice.md`

**Learning Objectives:**

**Part 1: Unit Testing**
- pytest fundamentals
- TestClient for endpoint testing
- Mocking LLM provider responses
- Mocking database calls
- pytest fixtures
- Parametrized tests
- Testing error scenarios

**Part 2: Integration Testing**
- Testing with real database (test DB)
- Testing Redis integration
- Testing async endpoints
- Testing streaming responses
- Testing authentication flows

**Part 3: Test Organization**
- Project structure (tests/ directory)
- Conftest.py for shared fixtures
- Test naming conventions
- Coverage reporting
- CI/CD integration basics

**Hands-On:**
- Exercise 1: Basic endpoint testing with TestClient
- Exercise 2: Mocking external LLM APIs
- Exercise 3: Database fixtures and cleanup
- Exercise 4: Testing authentication
- Exercise 5: Testing error handlers
- Exercise 6: Integration tests for chat API
- Exercise 7: Testing streaming endpoints
- Exercise 8: Achieving 80%+ code coverage

**ML Engineer Analogy:** Unit tests = data validation in pipelines. Integration tests = end-to-end pipeline testing in staging environment.

**Estimated Time:** 3 hours concepts + 5 hours hands-on

---

#### üîß **Mini-Project 1: Chat API with Database & Tests**
**Target Directory:** `projects/01_chat_api_with_db/`

**Objective:** Build a complete chat API with PostgreSQL persistence, Redis caching, and comprehensive tests.

**Features:**
- User management (create, get user)
- Conversation CRUD (create, list, get, delete)
- Message storage (save messages, retrieve history)
- Pagination for conversation history
- Redis caching for recent conversations
- Error handling with custom exceptions
- Comprehensive test suite (80%+ coverage)

**Tech Stack:**
- FastAPI
- PostgreSQL with SQLAlchemy (async)
- Redis for caching
- Alembic for migrations
- pytest for testing
- Docker Compose for local dev

**Deliverables:**
- Fully functional API
- Database schema with migrations
- Test suite with fixtures
- README with setup instructions
- docker-compose.yml for local development

**Estimated Time:** 8-10 hours

---

### **PHASE 4: LLM Integration Basics** ü§ñ **REAL PROVIDERS**

#### üìñ Topic 9: Integrating Real LLM Providers
**Target Files:**
- `concepts/09_LLM_Integration.md`
- `concepts/09_LLM_Integration_Practice.md`

**Learning Objectives:**

**Part 1: Provider SDKs**
- Anthropic SDK (claude-sonnet-4)
- OpenAI SDK (gpt-4)
- API key management (environment variables, secrets)
- Making API calls
- Handling responses
- Error handling (rate limits, timeouts, API errors)

**Part 2: Unified Interface**
- Creating provider abstraction layer
- Normalizing request/response formats
- Parameter translation (max_tokens differences)
- Provider-specific quirks
- Factory pattern for provider selection

**Part 3: LiteLLM Integration**
- LiteLLM setup
- Unified provider interface
- Automatic fallback
- Cost tracking
- Token counting

**Hands-On:**
- Exercise 1: Anthropic SDK integration
- Exercise 2: OpenAI SDK integration
- Exercise 3: Creating unified provider interface
- Exercise 4: LiteLLM integration
- Exercise 5: Provider selection endpoint
- Exercise 6: Error handling for API failures
- Exercise 7: Token usage tracking
- Exercise 8: Multi-provider chat API

**ML Engineer Analogy:** Provider abstraction = feature store interface. Different backends (Feast, Tecton) but same API for feature retrieval.

**Estimated Time:** 3 hours concepts + 5 hours hands-on

---

#### üìñ Topic 10: Streaming Responses & Server-Sent Events
**Target Files:**
- `concepts/10_Streaming.md`
- `concepts/10_Streaming_Practice.md`

**Learning Objectives:**

**Part 1: Streaming Fundamentals**
- Why streaming matters for LLMs (UX, perceived latency)
- Server-Sent Events (SSE) vs WebSockets
- StreamingResponse in FastAPI
- Async generators in Python
- Yielding data chunks

**Part 2: LLM Streaming**
- Streaming with Anthropic SDK
- Streaming with OpenAI SDK
- Handling stream errors mid-response
- Client disconnection detection
- Backpressure handling

**Part 3: Streaming + Database**
- Saving streamed messages
- Token counting on streamed responses
- Partial response recovery

**Hands-On:**
- Exercise 1: Basic StreamingResponse
- Exercise 2: Streaming from Anthropic
- Exercise 3: Streaming from OpenAI
- Exercise 4: Error handling in streams
- Exercise 5: Saving streamed responses to database
- Exercise 6: Multi-provider streaming chat API

**ML Engineer Analogy:** Streaming = processing Spark DataFrames in micro-batches instead of waiting for full dataset. Better UX, lower memory.

**Estimated Time:** 2.5 hours concepts + 4 hours hands-on

---

#### üîß **Mini-Project 2: Multi-Provider Chat with History**
**Target Directory:** `projects/02_multi_provider_chat/`

**Objective:** Build a production-quality chat API with multiple LLM providers, streaming responses, and conversation history.

**Features:**
- Multi-provider support (Anthropic, OpenAI, LiteLLM)
- Streaming responses with SSE
- Conversation history storage (PostgreSQL)
- Message persistence during streaming
- Token usage tracking per message
- Provider selection via API
- Error handling and fallback
- Redis caching for recent conversations
- Comprehensive tests

**Tech Stack:**
- FastAPI with async
- Anthropic SDK + OpenAI SDK + LiteLLM
- PostgreSQL (conversations + messages)
- Redis (caching)
- pytest with mocked providers
- Docker Compose

**Deliverables:**
- Multi-provider chat API
- Streaming endpoint
- Database schema with migrations
- Test suite (mocked LLM calls)
- API documentation
- docker-compose.yml

**Estimated Time:** 10-12 hours

---

### **PHASE 5: Authentication & Security** üîí

#### üìñ Topic 11: Authentication & API Keys
**Target Files:**
- `concepts/11_Authentication.md`
- `concepts/11_Authentication_Practice.md`

**Learning Objectives:**

**Part 1: Authentication Strategies**
- API keys vs JWT vs OAuth2
- When to use each strategy
- Stateless authentication for horizontal scaling

**Part 2: API Key Authentication**
- Generating secure API keys
- Storing hashed keys in database
- API key validation middleware
- Header-based authentication (Authorization: Bearer)
- Dependency injection for authentication

**Part 3: JWT Tokens**
- JWT structure (header, payload, signature)
- Creating JWTs with python-jose
- Validating JWTs
- Token expiration and refresh
- User authentication flow

**Part 4: FastAPI Security Utilities**
- OAuth2PasswordBearer
- HTTPBearer
- SecurityScopes
- Dependency injection patterns

**Hands-On:**
- Exercise 1: API key generation and storage
- Exercise 2: API key validation middleware
- Exercise 3: JWT token creation
- Exercise 4: JWT validation with dependencies
- Exercise 5: User login/signup endpoints
- Exercise 6: Protected endpoints
- Exercise 7: Role-based access control (RBAC)
- Exercise 8: Secure chat API with authentication

**ML Engineer Analogy:** API keys = Databricks tokens. JWT = session tokens for ML platforms. Always stateless so load balancers can route anywhere.

**Estimated Time:** 3 hours concepts + 5 hours hands-on

---

#### üìñ Topic 12: Rate Limiting & Abuse Prevention
**Target Files:**
- `concepts/12_Rate_Limiting.md`
- `concepts/12_Rate_Limiting_Practice.md`

**Learning Objectives:**

**Part 1: Rate Limiting Strategies**
- Why rate limiting matters (cost, abuse, fairness)
- Fixed window vs sliding window vs token bucket
- Rate limiting by user, IP, API key
- Rate limiting per endpoint

**Part 2: Redis-Based Rate Limiting**
- Implementing sliding window with Redis
- Token bucket algorithm
- Rate limit headers (X-RateLimit-Remaining)
- Handling rate limit errors (429 Too Many Requests)

**Part 3: Provider-Specific Rate Limits**
- Anthropic rate limits (RPM, TPM)
- OpenAI rate limits
- Coordinating rate limits across providers
- Queue management when limits exhausted

**Part 4: Usage Tracking**
- Token usage per user
- Cost calculation
- Budget limits and alerts
- Billing integration

**Hands-On:**
- Exercise 1: Implementing fixed window rate limiting
- Exercise 2: Sliding window with Redis
- Exercise 3: Token bucket algorithm
- Exercise 4: Rate limit middleware
- Exercise 5: Per-user token tracking
- Exercise 6: Cost calculation and budget enforcement
- Exercise 7: Rate-limited multi-provider API

**ML Engineer Analogy:** Rate limiting = cluster resource quotas. Token bucket = Spark dynamic allocation. Prevent one user from consuming all resources.

**Estimated Time:** 2.5 hours concepts + 4 hours hands-on

---

#### üîß **Mini-Project 3: Secure Chat API with Usage Limits**
**Target Directory:** `projects/03_secure_chat_api/`

**Objective:** Add authentication, authorization, rate limiting, and usage tracking to your multi-provider chat API.

**Features:**
- User signup and login with JWT
- API key authentication
- Protected endpoints (JWT or API key)
- Rate limiting (per user, per endpoint)
- Token usage tracking
- Cost calculation per request
- User budget limits
- Usage analytics endpoint
- Comprehensive tests with auth mocking

**Tech Stack:**
- FastAPI with OAuth2PasswordBearer
- JWT with python-jose
- Redis for rate limiting + caching
- PostgreSQL (users, API keys, usage logs)
- pytest with auth fixtures

**Deliverables:**
- Secure chat API with auth
- Rate limiting system
- Usage tracking and analytics
- Admin endpoints (usage reports)
- Test suite
- API documentation with auth examples

**Estimated Time:** 10-12 hours

---

### **PHASE 6: Advanced LLM Patterns** üöÄ **PRODUCTION GENAI**

#### üìñ Topic 13: Advanced Multi-Provider Patterns
**Target Files:**
- `concepts/13_Advanced_Multi_Provider.md`
- `concepts/13_Advanced_Multi_Provider_Practice.md`

**Learning Objectives:**

**Part 1: Provider Failover**
- Circuit breaker pattern
- Automatic provider fallback
- Provider health monitoring
- Fallback priority lists
- Error classification (retriable vs non-retriable)

**Part 2: Cost Optimization**
- Token counting before requests
- Cost tracking per provider
- Cheapest-provider routing
- Cost vs quality tradeoffs
- Budget-aware routing

**Part 3: Load Distribution**
- Round-robin provider selection
- Weighted load balancing
- Provider-specific rate limit coordination
- Queue management

**Part 4: Provider-Specific Features**
- Anthropic prompt caching (save 90% on repeated prompts)
- OpenAI function calling
- Exposing provider-specific features in unified API
- Feature detection and graceful degradation

**Hands-On:**
- Exercise 1: Circuit breaker implementation
- Exercise 2: Provider health checks
- Exercise 3: Automatic failover logic
- Exercise 4: Cost-aware routing
- Exercise 5: Token counting and cost estimation
- Exercise 6: Anthropic prompt caching integration
- Exercise 7: Complete resilient multi-provider system

**ML Engineer Analogy:** Circuit breaker = retrying failed Spark jobs with exponential backoff. Cost routing = choosing spot instances vs on-demand.

**Estimated Time:** 3 hours concepts + 5 hours hands-on

---

#### üìñ Topic 14: GenAI-Specific Patterns
**Target Files:**
- `concepts/14_GenAI_Patterns.md`
- `concepts/14_GenAI_Patterns_Practice.md`

**Learning Objectives:**

**Part 1: Prompt Management**
- Prompt templates with Jinja2
- Prompt versioning
- A/B testing prompts
- Prompt injection prevention
- System prompt management

**Part 2: Context Window Management**
- Token counting strategies (tiktoken)
- Conversation truncation algorithms
- Sliding window patterns
- Summarization for long contexts
- Context priority (keep system + recent messages)

**Part 3: Response Validation & Safety**
- Content filtering
- PII detection and redaction
- Moderation API integration
- Response quality checks
- Hallucination detection strategies

**Part 4: Caching Strategies**
- Semantic caching (similar prompts)
- Exact match caching
- Anthropic prompt caching (repeated system prompts)
- Redis caching patterns
- Cache invalidation strategies
- TTL decisions

**Part 5: Batch Processing**
- Batch API endpoints
- Async job queues (Celery/RQ/Dramatiq)
- Long-running task handling
- Progress tracking
- Result retrieval

**Hands-On:**
- Exercise 1: Prompt templates with Jinja2
- Exercise 2: Token counting and truncation
- Exercise 3: Sliding window context management
- Exercise 4: PII detection with regex/ML
- Exercise 5: Semantic caching with embeddings
- Exercise 6: Anthropic prompt caching
- Exercise 7: Batch processing with background tasks
- Exercise 8: Complete GenAI API with all patterns

**ML Engineer Analogy:** Prompt templates = feature engineering templates. Context management = managing DataFrame memory. Semantic caching = feature store caching.

**Estimated Time:** 4 hours concepts + 6 hours hands-on

---

#### üìñ Topic 15: Function Calling & Tool Use
**Target Files:**
- `concepts/15_Function_Calling.md`
- `concepts/15_Function_Calling_Practice.md`

**Learning Objectives:**

**Part 1: Function Calling Basics**
- What is function calling (tool use)?
- OpenAI function calling format
- Anthropic tool use format
- Defining tools/functions
- Tool choice (auto, required, specific)

**Part 2: Implementing Tools**
- Creating tool definitions
- Tool execution layer
- Result formatting
- Multi-turn tool use
- Error handling in tools

**Part 3: Common Tool Patterns**
- Database query tools
- API calling tools
- Code execution tools
- File system tools
- Calculator/math tools

**Part 4: Agentic Patterns**
- ReAct pattern (Reason + Act)
- Multi-step reasoning
- Tool chaining
- Autonomous agents vs assistive agents
- Loop prevention and safeguards

**Hands-On:**
- Exercise 1: Defining tools (calculator, weather)
- Exercise 2: OpenAI function calling
- Exercise 3: Anthropic tool use
- Exercise 4: Database query tool
- Exercise 5: Multi-turn conversations with tools
- Exercise 6: ReAct agent implementation
- Exercise 7: Tool error handling
- Exercise 8: Complete agentic assistant

**ML Engineer Analogy:** Tools = user-defined functions (UDFs) in Spark. Agents = orchestration workflows. ReAct = iterative model refinement.

**Estimated Time:** 3 hours concepts + 5 hours hands-on

---

#### üîß **Mini-Project 4: Agentic RAG System with Tools**
**Target Directory:** `projects/04_agentic_rag/`

**Objective:** Build a Retrieval-Augmented Generation (RAG) system with tool use, allowing the LLM to query databases, search documents, and perform calculations.

**Features:**
- Document ingestion and embedding
- Vector database integration (Pinecone or Qdrant)
- Semantic search tool
- SQL query tool (text-to-SQL)
- Calculator tool
- Weather API tool
- Multi-turn agentic conversations
- Tool execution monitoring
- Conversation history with tool calls
- Comprehensive tests

**Tech Stack:**
- FastAPI
- Anthropic SDK (tool use)
- Vector database (Pinecone/Qdrant)
- Embedding model (OpenAI text-embedding-3-small)
- PostgreSQL for structured data
- Redis for caching
- pytest

**Deliverables:**
- RAG API with semantic search
- Tool-use enabled chat endpoint
- Document ingestion pipeline
- Vector search implementation
- Agentic loop with safeguards
- Test suite with mocked tools
- API documentation

**Estimated Time:** 12-15 hours

---

### **PHASE 7: Production Deployment** üè≠ **DOCKER + INFRA**

#### üìñ Topic 16: Production Docker
**Target Files:**
- `concepts/16_Production_Docker.md`
- `concepts/16_Production_Docker_Practice.md`

**Learning Objectives:**

**Part 1: Multi-Stage Builds**
- Why multi-stage builds (image size, security)
- Builder stage vs runtime stage
- Copying only necessary artifacts
- Layer optimization strategies

**Part 2: Security Hardening**
- Running as non-root user
- .dockerignore file
- Scanning for vulnerabilities
- Minimal base images (python:3.11-slim)
- Secrets management (build args vs runtime)

**Part 3: Production Dockerfile Patterns**
- Dependency caching layers
- Virtual environment in container
- Entry point scripts
- Health check instructions
- Multi-architecture builds (ARM + x86)

**Part 4: Docker Compose for Production**
- Production override files (docker-compose.prod.yml)
- Resource limits (memory, CPU)
- Restart policies (unless-stopped)
- Log drivers and log rotation
- Networking configuration
- Volume management

**Hands-On:**
- Exercise 1: Converting basic Dockerfile to multi-stage
- Exercise 2: Security hardening (non-root user, .dockerignore)
- Exercise 3: Optimizing layer caching
- Exercise 4: Production docker-compose.yml
- Exercise 5: Health check implementation
- Exercise 6: Complete production Docker setup

**ML Engineer Analogy:** Multi-stage = training image vs serving image. Layer caching = caching Maven/pip dependencies. Resource limits = Databricks cluster sizing.

**Estimated Time:** 2 hours concepts + 4 hours hands-on

---

#### üìñ Topic 17: Production Deployment
**Target Files:**
- `concepts/17_Production_Deployment.md`
- `concepts/17_Production_Deployment_Practice.md`

**Learning Objectives:**

**Part 1: Gunicorn + Uvicorn Workers**
- Why multiple workers (concurrency, fault tolerance)
- Worker types (sync, async, gevent)
- Worker count calculation (2-4 x CPU cores)
- Worker lifecycle and restarts
- Graceful shutdown (SIGTERM handling)

**Part 2: NGINX as Reverse Proxy**
- Why NGINX (SSL termination, static files, load balancing)
- NGINX configuration for FastAPI
- Proxy headers (X-Forwarded-For, X-Real-IP)
- Request buffering
- Timeout configuration
- Rate limiting at NGINX level

**Part 3: Health Checks & Readiness**
- Liveness vs readiness probes
- Health check endpoint design
- Dependency health checks (DB, Redis, LLM providers)
- Startup probes for slow initialization

**Part 4: Zero-Downtime Deployments**
- Rolling deployments
- Blue-green deployments
- Canary deployments
- Connection draining
- Handling in-flight requests

**Part 5: Environment Configuration**
- 12-factor app principles
- Environment variables hierarchy
- Secrets management (AWS Secrets Manager, Vault)
- Configuration validation on startup

**Hands-On:**
- Exercise 1: Multi-worker Uvicorn setup
- Exercise 2: Gunicorn with Uvicorn workers
- Exercise 3: NGINX configuration
- Exercise 4: Comprehensive health checks
- Exercise 5: Graceful shutdown implementation
- Exercise 6: Rolling deployment with docker-compose
- Exercise 7: Complete production deployment

**ML Engineer Analogy:** Workers = Spark executors. NGINX = API Gateway. Health checks = cluster health monitoring. Zero-downtime = Databricks cluster resizing without job failure.

**Estimated Time:** 3 hours concepts + 5 hours hands-on

---

#### üìñ Topic 18: Monitoring & Metrics
**Target Files:**
- `concepts/18_Monitoring.md`
- `concepts/18_Monitoring_Practice.md`

**Learning Objectives:**

**Part 1: Structured Logging**
- JSON logging format
- Log levels (DEBUG, INFO, WARNING, ERROR, CRITICAL)
- Context injection (request_id, user_id)
- Logging LLM requests and responses
- Sensitive data redaction
- Log aggregation (CloudWatch, ELK stack)

**Part 2: Metrics with Prometheus**
- Prometheus architecture (scraping, TSDB)
- Metric types (Counter, Gauge, Histogram, Summary)
- Key metrics to track:
  - Request latency (p50, p95, p99)
  - Request rate (requests per second)
  - Error rate (4xx, 5xx)
  - Tokens per second
  - Provider success/failure rates
  - Cache hit rates
  - Cost per request
- Prometheus client library in FastAPI
- Exposing /metrics endpoint
- Grafana dashboards

**Part 3: Application Performance Monitoring (APM)**
- Sentry for error tracking
- DataDog/New Relic integration
- Exception aggregation
- Performance profiling
- User impact analysis

**Part 4: Business Metrics**
- Token usage trends
- Cost analysis per user/provider
- User engagement metrics
- Provider performance comparison
- Feature adoption tracking

**Part 5: Alerting**
- Alert rules (latency > threshold, error rate spike)
- Alert routing (PagerDuty, Opsgenie, Slack)
- Cost alerts (daily spend > budget)
- Provider downtime alerts
- Alert fatigue prevention

**Hands-On:**
- Exercise 1: Structured JSON logging
- Exercise 2: Request ID middleware and logging
- Exercise 3: Prometheus metrics in FastAPI
- Exercise 4: Custom metrics (tokens, cost)
- Exercise 5: Grafana dashboard creation
- Exercise 6: Sentry integration
- Exercise 7: Alert rules in Prometheus
- Exercise 8: Complete observability stack

**ML Engineer Analogy:** Metrics = Spark UI metrics. Prometheus = Databricks job monitoring. Structured logging = Delta audit logs. Alerting = data quality alerts.

**Estimated Time:** 3 hours concepts + 5 hours hands-on

---

#### üîß **Mini-Project 5: Production-Ready Deployment**
**Target Directory:** `projects/05_production_deployment/`

**Objective:** Deploy your agentic RAG system with production-grade infrastructure: multi-worker setup, NGINX, monitoring, and health checks.

**Features:**
- Multi-stage Docker build
- Multi-worker Gunicorn + Uvicorn
- NGINX reverse proxy with SSL
- Comprehensive health checks
- Structured JSON logging
- Prometheus metrics
- Grafana dashboard
- Sentry error tracking
- Docker Compose production setup
- Graceful shutdown
- Environment variable configuration
- Secrets management

**Tech Stack:**
- FastAPI app from Project 4
- Docker multi-stage build
- Gunicorn + Uvicorn workers
- NGINX
- Prometheus + Grafana
- Sentry
- PostgreSQL + Redis
- docker-compose.prod.yml

**Deliverables:**
- Production Dockerfile
- NGINX configuration
- docker-compose.prod.yml
- Prometheus configuration
- Grafana dashboard JSON
- Health check implementation
- Deployment documentation
- Monitoring runbook

**Estimated Time:** 10-12 hours

---

### **PHASE 8: Advanced Topics** üéì **ENTERPRISE-READY**

#### üìñ Topic 19: Vector Databases for RAG
**Target Files:**
- `concepts/19_Vector_Databases.md`
- `concepts/19_Vector_Databases_Practice.md`

**Learning Objectives:**

**Part 1: Vector Database Fundamentals**
- What are embeddings?
- Vector similarity search (cosine, euclidean, dot product)
- When to use vector databases
- Pinecone vs Qdrant vs Weaviate vs Chroma

**Part 2: Pinecone Integration**
- Pinecone setup and API keys
- Creating indexes
- Upserting vectors with metadata
- Querying with filters
- Namespaces for multi-tenancy
- Hybrid search (vector + keyword)

**Part 3: Qdrant Integration**
- Qdrant setup (cloud vs self-hosted)
- Collections and points
- Vector search with payload filtering
- Scalar quantization for efficiency
- HNSW parameters tuning

**Part 4: RAG Patterns**
- Document chunking strategies
- Embedding generation (OpenAI, Cohere)
- Retrieval with metadata filtering
- Reranking results
- Context injection into prompts
- Hybrid search (semantic + keyword)
- RAG evaluation metrics

**Hands-On:**
- Exercise 1: Generating embeddings with OpenAI
- Exercise 2: Pinecone index creation and upsertion
- Exercise 3: Semantic search queries
- Exercise 4: Qdrant integration
- Exercise 5: Document chunking strategies
- Exercise 6: RAG endpoint implementation
- Exercise 7: Hybrid search with filtering
- Exercise 8: Complete RAG API

**ML Engineer Analogy:** Vector DB = specialized index for high-dimensional data. Embeddings = feature vectors. Similarity search = KNN in feature space.

**Estimated Time:** 3 hours concepts + 5 hours hands-on

---

#### üìñ Topic 20: Distributed Tracing
**Target Files:**
- `concepts/20_Distributed_Tracing.md`
- `concepts/20_Distributed_Tracing_Practice.md`

**Learning Objectives:**

**Part 1: Distributed Tracing Concepts**
- Why tracing matters (debugging async, microservices)
- Traces, spans, and context propagation
- OpenTelemetry architecture
- Sampling strategies

**Part 2: OpenTelemetry Setup**
- OpenTelemetry SDK setup
- Auto-instrumentation for FastAPI
- Manual span creation
- Span attributes and events
- Context propagation across async calls

**Part 3: Tracing LLM Calls**
- Creating spans for LLM requests
- Recording token usage in spans
- Tracing multi-provider fallback
- Tracing RAG pipeline (retrieval + generation)
- Error tracking in traces

**Part 4: Trace Backends**
- Jaeger setup
- Grafana Tempo
- DataDog APM
- Visualizing traces
- Query and analysis

**Hands-On:**
- Exercise 1: OpenTelemetry setup in FastAPI
- Exercise 2: Auto-instrumentation
- Exercise 3: Manual span creation
- Exercise 4: Tracing LLM API calls
- Exercise 5: Tracing RAG pipeline
- Exercise 6: Jaeger integration
- Exercise 7: Analyzing traces for performance
- Exercise 8: Complete traced GenAI API

**ML Engineer Analogy:** Distributed tracing = Spark DAG visualization. Spans = Spark stages. Context propagation = passing job context through transformations.

**Estimated Time:** 2.5 hours concepts + 4 hours hands-on

---

#### üìñ Topic 21: Container Orchestration (Kubernetes Basics)
**Target Files:**
- `concepts/21_Kubernetes_Basics.md`
- `concepts/21_Kubernetes_Basics_Practice.md`

**Learning Objectives:**

**Part 1: Kubernetes Fundamentals**
- Why Kubernetes (scaling, self-healing, declarative)
- Kubernetes architecture (control plane, nodes)
- kubectl basics
- Namespaces

**Part 2: Core Resources**
- Pods (containers)
- Deployments (replica sets)
- Services (load balancing)
- ConfigMaps (configuration)
- Secrets (sensitive data)
- Ingress (routing)

**Part 3: Deploying FastAPI to Kubernetes**
- Writing Deployment manifests
- Service configuration (ClusterIP, LoadBalancer)
- ConfigMap for environment variables
- Secrets for API keys
- Health checks (liveness, readiness, startup)
- Resource limits and requests

**Part 4: Scaling and Updates**
- Horizontal Pod Autoscaler (HPA)
- Vertical Pod Autoscaler (VPA)
- Rolling updates
- Rollback strategies

**Part 5: Persistence**
- PersistentVolumeClaims
- StatefulSets for databases
- Connecting to external databases (RDS, CloudSQL)

**Hands-On:**
- Exercise 1: Local Kubernetes with Minikube/Kind
- Exercise 2: Writing Deployment manifests
- Exercise 3: Exposing services
- Exercise 4: ConfigMaps and Secrets
- Exercise 5: Health checks
- Exercise 6: Horizontal Pod Autoscaler
- Exercise 7: Rolling updates
- Exercise 8: Complete K8s deployment for GenAI API

**ML Engineer Analogy:** Kubernetes = YARN for containerized apps. Pods = containers. Deployments = Databricks clusters. HPA = cluster autoscaling. Services = load balancers.

**Estimated Time:** 4 hours concepts + 6 hours hands-on

---

#### üìñ Topic 22: LangChain & LangServe Integration (Optional/Advanced)
**Target Files:**
- `concepts/22_LangServe_Integration.md`
- `concepts/22_LangServe_Integration_Practice.md`

**Learning Objectives:**

**Part 1: LangServe Fundamentals**
- What is LangServe? (FastAPI wrapper for LangChain)
- When to use LangServe vs raw FastAPI
- Auto-generated endpoints: `/invoke`, `/batch`, `/stream`, `/stream_log`
- Schema inference from LangChain objects
- Playground UI for testing

**Part 2: Deploying LangChain Runnables**
- Converting chains to deployable APIs
- RunnablePassthrough for preprocessing
- RunnableLambda for custom logic
- Parallel runnable composition
- Error handling in chains

**Part 3: LangServe Patterns**
```python
from fastapi import FastAPI
from langserve import add_routes
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate

app = FastAPI()

# Create a chain
prompt = ChatPromptTemplate.from_template("Tell me about {topic}")
chain = prompt | ChatAnthropic(model="claude-sonnet-4-20250514")

# Auto-generate /invoke, /batch, /stream endpoints
add_routes(app, chain, path="/chat")
```

**Part 4: Configurable Runnables**
- Multi-tenant configurations
- Runtime parameter injection
- `configurable_fields` for dynamic behavior
- User-specific model selection
- Per-request configuration override

**Part 5: LangSmith Integration**
- Tracing LangServe deployments
- Observability for production chains
- Debugging chain failures
- Cost tracking per trace
- Feedback collection

**Part 6: Production Considerations**
- LangServe vs custom FastAPI: tradeoffs
- Performance benchmarking
- Versioning deployed chains
- A/B testing prompts via configuration
- Combining LangServe with custom endpoints

**Hands-On:**
- Exercise 1: Basic LangServe deployment
- Exercise 2: Creating configurable runnables
- Exercise 3: Streaming with LangServe
- Exercise 4: Multi-tenant configuration
- Exercise 5: LangSmith tracing integration
- Exercise 6: Hybrid app (LangServe + custom FastAPI)
- Exercise 7: Complete LangServe-powered GenAI API

**ML Engineer Analogy:** LangServe = MLflow model serving for LLM chains. It standardizes the deployment pattern just like MLflow standardizes model packaging. Custom FastAPI = building your own serving infrastructure.

**When to Use LangServe:**
- Rapid prototyping and deployment
- Teams already using LangChain
- Standard chat/RAG patterns
- Need for auto-generated playground

**When to Use Raw FastAPI:**
- Maximum control over request/response
- Complex multi-provider orchestration
- Non-standard streaming patterns
- Existing FastAPI infrastructure

**Note:** LangGraph (newer) is evolving as an alternative for more complex agent workflows. This topic focuses on LangServe for simpler chain deployments.

**Estimated Time:** 2.5 hours concepts + 4 hours hands-on

---

#### üîß **Final Project: Enterprise GenAI Platform**
**Target Directory:** `projects/06_enterprise_platform/`

**Objective:** Build a complete, enterprise-ready GenAI platform with all production features: multi-provider LLM support, RAG, function calling, authentication, rate limiting, monitoring, and Kubernetes deployment.

**Features:**

**Core Functionality:**
- Multi-provider LLM chat (Anthropic, OpenAI, LiteLLM)
- Streaming responses with SSE
- RAG with vector database (Pinecone/Qdrant)
- Function calling / tool use
- Conversation history (PostgreSQL)
- User management and authentication (JWT + API keys)
- Rate limiting per user/endpoint
- Token usage tracking and billing

**Production Infrastructure:**
- Multi-stage Docker builds
- Kubernetes deployment manifests
- Horizontal Pod Autoscaler
- Ingress with SSL termination
- Health checks (liveness, readiness)
- Graceful shutdown

**Observability:**
- Structured JSON logging
- Prometheus metrics (latency, tokens, cost, errors)
- Grafana dashboards
- Distributed tracing (OpenTelemetry + Jaeger)
- Sentry error tracking
- Alerting rules

**Advanced Patterns:**
- Circuit breaker for providers
- Automatic failover
- Semantic caching
- Prompt caching (Anthropic)
- Context window management
- PII detection and redaction
- Content moderation

**Testing & Quality:**
- Comprehensive test suite (80%+ coverage)
- Integration tests
- Load tests (Locust)
- CI/CD pipeline (GitHub Actions)

**Tech Stack:**
- FastAPI with async
- PostgreSQL + Redis
- Vector database (Pinecone/Qdrant)
- Anthropic + OpenAI + LiteLLM
- Docker + Kubernetes
- Prometheus + Grafana + Jaeger + Sentry
- pytest + Locust
- GitHub Actions

**Deliverables:**
- Complete GenAI platform codebase
- Kubernetes manifests (deployment, service, ingress, HPA)
- Monitoring stack configuration
- Comprehensive API documentation
- Test suite with high coverage
- Load test scripts and benchmarks
- CI/CD pipeline
- Deployment runbook
- Architecture diagram
- Cost analysis document

**Estimated Time:** 20-25 hours

---

## üìä Progress Tracking

### Current Status
- ‚úÖ Phase 1: Foundation (Topics 1-2 + Advanced Extensions) - **COMPLETED**
- üéØ Phase 2: Backend Essentials (Topics 3-6) - **NEXT** *(includes new Project Structure topic)*
- ‚è≥ Phase 3: Production Foundations (Topics 7-8 + Project 1)
- ‚è≥ Phase 4: LLM Integration Basics (Topics 9-10 + Project 2)
- ‚è≥ Phase 5: Authentication & Security (Topics 11-12 + Project 3)
- ‚è≥ Phase 6: Advanced LLM Patterns (Topics 13-15 + Project 4)
- ‚è≥ Phase 7: Production Deployment (Topics 16-18 + Project 5)
- ‚è≥ Phase 8: Advanced Topics (Topics 19-22 + Final Project) *(includes new LangServe topic)*

### Topics Summary
| Phase | Topics | New Additions |
|-------|--------|---------------|
| Phase 1 | 2 core + 2 advanced extensions | Custom Base Model, Advanced Dependencies |
| Phase 2 | 4 topics | **Project Structure & Organization** |
| Phase 3-7 | 12 topics | *(renumbered)* |
| Phase 8 | 4 topics | **LangChain & LangServe Integration** |
| **Total** | **22 topics** | +2 new topics added |

---

## üéØ Success Metrics

By the end of this roadmap, you will be able to:

**Technical Skills:**
- ‚úÖ Build production-grade FastAPI backends
- ‚úÖ Integrate multiple LLM providers with fallback strategies
- ‚úÖ Implement RAG systems with vector databases
- ‚úÖ Build agentic systems with tool use
- ‚úÖ Deploy to production with Docker and Kubernetes
- ‚úÖ Implement authentication, rate limiting, and security best practices
- ‚úÖ Monitor and debug production issues with distributed tracing
- ‚úÖ Write comprehensive tests with high coverage
- ‚úÖ Optimize costs across multiple LLM providers

**Production Readiness:**
- ‚úÖ Zero-downtime deployments
- ‚úÖ Horizontal scaling with load balancing
- ‚úÖ Observability (logging, metrics, tracing)
- ‚úÖ Error handling and resilience patterns
- ‚úÖ Security hardening (auth, rate limiting, input validation)
- ‚úÖ Cost tracking and optimization
- ‚úÖ Performance optimization (caching, async)

**GenAI Expertise:**
- ‚úÖ Multi-provider abstraction and cost optimization
- ‚úÖ Streaming responses for better UX
- ‚úÖ Prompt management and versioning
- ‚úÖ Context window management
- ‚úÖ RAG implementation with vector databases
- ‚úÖ Function calling and tool use
- ‚úÖ Agentic patterns (ReAct, multi-step reasoning)
- ‚úÖ Content safety and PII detection

---

## üìö Learning Resources

### Official Documentation
- [FastAPI Docs](https://fastapi.tiangolo.com/)
- [Pydantic Docs](https://docs.pydantic.dev/)
- [Uvicorn Docs](https://www.uvicorn.org/)
- [SQLAlchemy Docs](https://docs.sqlalchemy.org/)
- [Anthropic API Docs](https://docs.anthropic.com/)
- [OpenAI API Docs](https://platform.openai.com/docs/)
- [Docker Docs](https://docs.docker.com/)
- [Kubernetes Docs](https://kubernetes.io/docs/)

### Books
- "FastAPI: Modern Python Web Development" by Bill Lubanovic
- "Architecting Modern Web Applications with FastAPI" (Packt)
- "Python Concurrency with asyncio" by Matthew Fowler

### Tools & Libraries
- **FastAPI:** Web framework
- **Uvicorn:** ASGI server
- **Gunicorn:** WSGI server (with Uvicorn workers)
- **Pydantic:** Data validation
- **SQLAlchemy:** ORM
- **Alembic:** Database migrations
- **Redis:** Caching and rate limiting
- **pytest:** Testing
- **Anthropic SDK:** Claude API
- **OpenAI SDK:** GPT API
- **LiteLLM:** Multi-provider abstraction
- **Pinecone/Qdrant:** Vector databases
- **Prometheus:** Metrics
- **Grafana:** Dashboards
- **Jaeger:** Distributed tracing
- **Sentry:** Error tracking
- **Docker:** Containerization
- **Kubernetes:** Orchestration

---

## üéì Learning Tips

### For Your Background (ML Engineer)
- **Leverage your expertise:** Connect every concept to your Databricks/Spark/AWS knowledge
- **Think distributed:** FastAPI backends scale horizontally like Spark clusters
- **Apply data thinking:** Treat HTTP requests like data pipeline events
- **Use analogies:** We'll consistently map to your ML infrastructure experience

### General Learning Principles
- **Hands-on first:** Type every example, break things, experiment
- **Progressive complexity:** Each topic builds on the previous
- **Test early, test often:** Write tests as you learn
- **Read others' code:** Explore FastAPI's GitHub, study production apps
- **Build real projects:** Don't skip the mini-projects‚Äîthey solidify learning
- **Deploy early:** Get comfortable with Docker/K8s early in the journey

### Time Management
- **Consistent schedule:** 10-15 hours/week is better than weekend binges
- **Theory + Practice:** Always do both concept reading and hands-on exercises
- **Project time:** Allocate dedicated time for mini-projects (weekends work well)
- **Review regularly:** Revisit earlier topics to reinforce learning

---

## üöÄ Next Steps

**You are currently at:** ‚úÖ Topics 1-2 completed

**Next immediate actions:**
1. **Review this roadmap** and bookmark it for reference
2. **Proceed to Topic 3:** Error Handling & Custom Responses
3. **Complete exercises 3-5** (Error Handling, Middleware, Async)
4. **Move to Phase 3:** Database Integration + Testing
5. **Build Mini-Project 1:** Chat API with Database & Tests
6. **Continue with LLM integration** in Phase 4

**Remember:** This is a marathon, not a sprint. Take your time with each topic, experiment, break things, and build solid foundations. By the end of this roadmap, you'll be able to build and deploy production-grade GenAI backends with confidence.

---

## üìù Notes & Customization

This roadmap is a living document. As you progress:
- Add your own notes and insights
- Mark topics as completed (change ‚è≥ to ‚úÖ)
- Adjust time estimates based on your pace
- Add additional resources you find helpful
- Create your own mini-projects based on specific needs

**Questions or stuck?** Refer back to concept files, revisit exercises, or experiment with variations.

---

**Good luck on your FastAPI mastery journey! üöÄ**
